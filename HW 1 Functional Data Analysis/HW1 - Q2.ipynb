{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161808a4",
   "metadata": {},
   "source": [
    "# HW1 - Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760f8e1",
   "metadata": {},
   "source": [
    "Consider the penalized least square problems where k is a positive integer:  \n",
    "$\\hat{f_1} = min_f [\\Sigma_{i=1}^{N}(y - f(x_i))^2 + \\lambda\\int_a^b(f^{(m)}(x))^2 dx]$  \n",
    "$\\hat{f_2} = min_f [\\Sigma_{i=1}^{N}(y - f(x_i))^2 + \\lambda\\int_a^b(f^{(m+k)}(x))^2 dx]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212984e",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**Consider $\\hat{f_1}$ in the following cases and determine the degree of the polynomial.**\n",
    "\n",
    "**1. m = 0 and $\\lambda = \\infty$**  \n",
    "When m = 0, $f^{(0)}(x) = f(x)$. As per the hint given in the Ed discussion board, when $\\lambda=\\infty$, $f^{(m)} \\rightarrow 0$ so $f^{(0)}(x) = f(x) = 0$ and has no degree.\n",
    "\n",
    "**2. m = 1 and $\\lambda = \\infty$**  \n",
    "When m = 1, $\\frac{\\partial f}{\\partial x} =  0$ and thus $f(x)$ must be a constant and have degree = 0.\n",
    "\n",
    "**3. m = 2 and $\\lambda = \\infty$**  \n",
    "When m = 2, $\\frac{\\partial^2 f}{\\partial x^2} = 0$ and thus $f(x)$ must be a line and have degree = 1.\n",
    "\n",
    "**4. m = 3 and $\\lambda = 0$**  \n",
    "If m = 3, $\\frac{\\partial^3 f}{\\partial 3^2} = 0$. However, since $\\lambda = 0$, we are interpolating $f(x)$ with no penalization for the curvature of the function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63415adf",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "**1. Will $\\hat{f_1}$ or $\\hat{f_2}$ have smaller training residual sum of squares as $\\lambda \\rightarrow \\infty$?**  \n",
    "Since $\\hat{f_2}$ has a higher degree than $\\hat{f_1}$ ($m+k \\gt m$), $\\hat{f_2}$ will overfit the training. Thus, $\\hat{f_2}$ will have the smaller residual sum of squares.\n",
    "  \n",
    "**2. Will $\\hat{f_1}$ or $\\hat{f_2}$ have smaller test residual sum of squares as $\\lambda \\rightarrow \\infty$?**  \n",
    "Since $\\hat{f_2}$ overfits the training, it will perform worse than $\\hat{f_1}$ using test data. Thus, $\\hat{f_1}$ will have the smaller residual sum of squares.\n",
    "\n",
    "**3. Will $\\hat{f_1}$ or $\\hat{f_2}$ have smaller training and test residual sum of squares for $\\lambda = 0$?**  \n",
    "If $\\lambda = 0$, the second term in both equations will disappear and $\\hat{f_1} = \\hat{f_2}$. Thus, $\\hat{f_1}$ and $\\hat{f_2}$ will have the same training and test residual sum of squares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
